# General-purpose settings.
verbose = true
logPath = log
logFilePostfix =
saveParticles = true

[plugins]
heuristicPlugin = libManipulatorRRTHeuristicPlugin.so

planningRewardPlugin = libdefaultRewardPlugin.so
executionRewardPlugin = libdefaultRewardPlugin.so

planningTerminalPlugin = libdefaultTerminalPlugin.so
executionTerminalPlugin = libdefaultTerminalPlugin.so

planningTransitionPlugin = libdefaultTransitionPlugin.so
executionTransitionPlugin = libdefaultTransitionPlugin.so

planningObservationPlugin = libdefaultObservationPlugin.so
executionObservationPlugin = libdefaultObservationPlugin.so

executionInitialBeliefPlugin = libdefaultInitialBeliefPlugin.so
planningInitialBeliefPlugin = libdefaultInitialBeliefPlugin.so

[transitionPluginOptions]
errorDistribution = UNIFORM
lowerUpperBoundUniformDistribution = [[-5.5, 10.5], [-10.5, 10.5]]
processError = 1.75
controlDuration = 0.1
softLimitThreshold = 0.03

[observationPluginOptions]
errorDistribution = UNIFORM
lowerUpperBoundUniformDistribution = [[-0.1, 0.1], [-0.1, 0.1]]
#lowerUpperBoundUniformDistribution = [[-0.1, 0.1]]
observationError = 0.075

[rewardPluginOptions]
stepPenalty = 1
illegalMovePenalty = 500
exitReward = 1000

[heuristicPluginOptions]
planningRange = 0.08

[initialBeliefOptions]
lowerBound = [0.0, 0.0, 0.0, 0.0]
upperBound = [0.0, 0.0, 0.0, 0.0]

[terminalPluginOptions]
goalLink = 2DOFManipulator::link2
goalLinkPoint = [1.0, 0, 0]

[heuristicPluginOptions]
planningRange = 1.0
goalState = [3.14, 0, 0, 0]

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 500

# The planning environment SDF
planningEnvironmentPath = 2DOFEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = 2DOFEnvironment.sdf

# The robot SDF model
robotName = 2DOFManipulator

enableGazeboStateLogging = true
overwriteExistingLogFiles = true

# The discount factor of the reward model
discountFactor = 0.99

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

[state]
jointPositions = [2DOFManipulator::joint1, 2DOFManipulator::joint2]
jointVelocities = [2DOFManipulator::joint1, 2DOFManipulator::joint2]

[action]
jointTorques = [2DOFManipulator::joint1, 2DOFManipulator::joint2]
#jointPositionIncrements = [2DOFManipulator::joint1, 2DOFManipulator::joint2]
#jointPositionIncrementLimits = [[-0.1, 0.1], [-0.1, 0.1]]

[observation]
jointPositions = [2DOFManipulator::joint1, 2DOFManipulator::joint2]

[changes]
hasChanges = false
changesPath = changes_1.txt
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 2000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchStrategy = ucb(2.0)

estimator = max()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 2

observationType = continuous

# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 0.5

[simulation]
interactive = false
particlePlotLimit = 100
