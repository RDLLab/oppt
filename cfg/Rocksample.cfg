# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =

# If this is set to 'true', the belief particles are written to the log file.
# WARNING: log files can become huge!
saveParticles = false

[plugins]
heuristicPlugin = libRocksampleHeuristicPlugin.so

planningRewardPlugin = librocksampleRewardPlugin.so
executionRewardPlugin = librocksampleRewardPlugin.so

planningTerminalPlugin = librocksampleTerminalPlugin.so
executionTerminalPlugin = librocksampleTerminalPlugin.so

planningTransitionPlugin = librocksampleTransitionPlugin.so
executionTransitionPlugin = librocksampleTransitionPlugin.so

planningObservationPlugin = librocksampleObservationPlugin.so
executionObservationPlugin = librocksampleObservationPlugin.so

executionInitialBeliefPlugin = librocksampleInitialBeliefPlugin.so
planningInitialBeliefPlugin = librocksampleInitialBeliefPlugin.so

[observationPluginOptions]
halfEfficiencyDistance = 20.0

[rewardPluginOptions]
# The immediate reward of entering the goal area at the right side of the map
exitReward = 10.0

# The immediate reward of sampling a good rock
goodRockSamplingReward = 10.0

# The immediate penalty of sampling a bad rock
badRockSamplingPenalty = -10.0

[initialBeliefOptions]
# The initial XY-coordinates of the rover
initialRoverPosition = [1.0, 4.0]

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 150

# The planning environment SDF
planningEnvironmentPath = RocksampleEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = RocksampleEnvironment.sdf

# The robot SDF model
robotName = RocksampleRobot

# The discount factor of the problem
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
# The state is defined as a 10-dimensional vector consisting of the XY-position of the rover
# and the states of the 8 rocks 
linkPositionsX = [RocksampleRobot::RocksampleLink]
linkPositionsY = [RocksampleRobot::RocksampleLink]

linkPositionXLimits = [[-1, 9]]
linkPositionYLimits = [[-1, 9]]

# Add 8 additional dimensions to the state that encode states of the 8 rocks
additionalDimensions = 8

# Rock states can either be "0" for bad rocks and "1" for good rocks
additionalDimensionLimits = [[0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]

[action]
# Here we construct a 1-dimensional action space with range 1-13. ABT will discretize the action space into 13 discrete actions
additionalDimensions = 1
additionalDimensionLimits = [[1, 13]]

[observation]
# Here we construct a 1-dimensional observation space with range 0-2. 
# ABT will discretize the observation space into 3 discrete observations {0, 1, 2}.
# For this problem, observation 0 is a surrogate for "no observation", whereas the observations 1 and 2 indicate the state of
# a rock that is being checked by the rover
additionalDimensions = 1
additionalDimensionLimits = [[0, 2]]

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 10000

# The maximum depth of the search tree 
maximumDepth = 3

# True if the above maximum depth is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

# Here we use UCB1 as the action selection strategy with an exploration constant of 5.0
searchStrategy = ucb(5.0)

#savePolicy = false
#loadInitialPolicy = false
#policyPath = final-0.pol

# This discretizes the 1-dimensional action space into 13 discrete actions
actionType = discrete
numInputStepsActions = 13

# This discretizes the 1-dimensional observation space into 3 discrete actions
observationType = discrete
numInputStepsObservations = 3

# If this is set to true, the policy will be recomputed from scratch after every step 
resetPolicy = false
